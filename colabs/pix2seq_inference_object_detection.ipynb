{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "#@title License\n",
    "# Copyright 2022 The Pix2Seq Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ],
   "metadata": {
    "cellView": "form",
    "id": "2i7FMjUnHtdx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pix2seq: A Language Modeling Framework for Object Detection\n",
    "<a href=\"https://colab.research.google.com/github/google-research/pix2seq/blob/master/colabs/pix2seq_inference_object_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "\n",
    "This colab presents a demo for object detection inference with Pix2seq. The table below provides a summary and model location for fine-tuned models on MSCOCO dataset.\n",
    "\n",
    "Backbone       | Total params (M) | Image size | COCO AP   | Google cloud storage location\n",
    "-------------: | ---------------: | ---------: | --------: | -----------:\n",
    "ResNet-50      | 36.6             | 640x640    | 39.1      | [gs://pix2seq/coco_det_finetune/resnet_640x640](https://console.cloud.google.com/storage/browser/pix2seq/coco_det_finetune/resnet_640x640)\n",
    "ResNet-50      | 36.6             | 1024x1024  | 41.7      | [gs://pix2seq/coco_det_finetune/resnet_1024x1024](https://console.cloud.google.com/storage/browser/pix2seq/coco_det_finetune/resnet_1024x1024)\n",
    "ResNet-50      | 36.6             | 1333x1333  | 42.6      | [gs://pix2seq/coco_det_finetune/resnet_1333x1333](https://console.cloud.google.com/storage/browser/pix2seq/coco_det_finetune/resnet_1333x1333)\n",
    "ResNet-50 (C4) | 84.7             | 640x640    | 44.7      | [gs://pix2seq/coco_det_finetune/resnetc_640x640](https://console.cloud.google.com/storage/browser/pix2seq/coco_det_finetune/resnetc_640x640)\n",
    "ResNet-50 (C4) | 84.7             | 1024x1024  | 46.9      | [gs://pix2seq/coco_det_finetune/resnetc_1024x1024](https://console.cloud.google.com/storage/browser/pix2seq/coco_det_finetune/resnetc_1024x1024)\n",
    "ResNet-50 (C4) | 84.7             | 1333x1333  | 47.3      | [gs://pix2seq/coco_det_finetune/resnetc_1333x1333](https://console.cloud.google.com/storage/browser/pix2seq/coco_det_finetune/resnetc_1333x1333)\n",
    "ViT-B          | 115.2            | 640x640    | 44.2      | [gs://pix2seq/coco_det_finetune/vit_b_640x640](https://console.cloud.google.com/storage/browser/pix2seq/coco_det_finetune/vit_b_640x640)\n",
    "ViT-B          | 115.2            | 1024x1024  | 46.5      | [gs://pix2seq/coco_det_finetune/vit_b_1024x1024](https://console.cloud.google.com/storage/browser/pix2seq/coco_det_finetune/vit_b_1024x1024)\n",
    "ViT-B          | 115.2            | 1333x1333  | 47.1      | [gs://pix2seq/coco_det_finetune/vit_b_1333x1333](https://console.cloud.google.com/storage/browser/pix2seq/coco_det_finetune/vit_b_1333x1333)\n",
    "ViT-L          | 341.2            | 640x640    | 47.6      | [gs://pix2seq/coco_det_finetune/vit_l_640x640](https://console.cloud.google.com/storage/browser/pix2seq/coco_det_finetune/vit_l_640x640)\n",
    "ViT-L          | 341.2            | 1024x1024  | 49.2      | [gs://pix2seq/coco_det_finetune/vit_l_1024x1024](https://console.cloud.google.com/storage/browser/pix2seq/coco_det_finetune/vit_l_1024x1024)\n",
    "ViT-L          | 341.2            | 1333x1333  | 50.0      | [gs://pix2seq/coco_det_finetune/vit_l_1333x1333](https://console.cloud.google.com/storage/browser/pix2seq/coco_det_finetune/vit_l_1333x1333)"
   ],
   "metadata": {
    "id": "O1J5i8CxIoVP"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dP71X32exIn",
    "outputId": "ab0b5115-ccb7-42dc-a859-340382540301",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "cellView": "form"
   },
   "source": [
    "#@title Imports.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "!pip install ml_collections\n",
    "!pip install tensorflow-addons\n",
    "!git clone https://github.com/google/pix2seq.git\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "root_dir = os.getcwd()\n",
    "sys.path.insert(1, 'pix2seq')\n",
    "\n",
    "import ml_collections\n",
    "from models import ar_model as model_lib\n",
    "from data import data_utils\n",
    "from tasks.object_detection import TaskObjectDetection\n",
    "from tasks.visualization import vis_utils"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZmmbMkOqNQi",
    "cellView": "form"
   },
   "source": [
    "#@title Load a model.\n",
    "model_dir = 'gs://pix2seq/coco_det_finetune/resnet_640x640' #@param [\"gs://pix2seq/coco_det_finetune/resnet_640x640\", \"gs://pix2seq/coco_det_finetune/resnet_1024x1024\", \"gs://pix2seq/coco_det_finetune/resnet_1333x1333\", \"gs://pix2seq/coco_det_finetune/resnetc_640x640\", \"gs://pix2seq/coco_det_finetune/resnetc_1024x1024\", \"gs://pix2seq/coco_det_finetune/resnetc_1333x1333\", \"gs://pix2seq/coco_det_finetune/vit_b_640x640\", \"gs://pix2seq/coco_det_finetune/vit_b_1024x1024\", \"gs://pix2seq/coco_det_finetune/vit_b_1333x1333\", \"gs://pix2seq/coco_det_finetune/vit_l_640x640\", \"gs://pix2seq/coco_det_finetune/vit_l_1024x1024\", \"gs://pix2seq/coco_det_finetune/vit_l_1333x1333\"] {allow-input: true}\n",
    "\n",
    "with tf.io.gfile.GFile(os.path.join(model_dir, 'config.json'), 'r') as f:\n",
    "  config = ml_collections.ConfigDict(json.loads(f.read()))\n",
    "\n",
    "# Set batch size to 1.\n",
    "config.eval.batch_size = 1\n",
    "\n",
    "# Remove the annotation filepaths.\n",
    "config.dataset.coco_annotations_dir = None\n",
    "\n",
    "# Update config fields.\n",
    "config.task.vocab_id = 10  # object_detection task vocab id.\n",
    "config.training = False\n",
    "config.dataset.val_filename='instances_val2017.json'\n",
    "\n",
    "assert config.task.name == \"object_detection\"\n",
    "task = TaskObjectDetection(config)\n",
    "\n",
    "# Restore checkpoint.\n",
    "model = model_lib.Model(config)\n",
    "checkpoint = tf.train.Checkpoint(\n",
    "    model=model, global_step=tf.Variable(0, dtype=tf.int64))\n",
    "ckpt = tf.train.latest_checkpoint(model_dir)\n",
    "checkpoint.restore(ckpt).expect_partial()\n",
    "global_step = checkpoint.global_step"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKqnnuvvvivu",
    "cellView": "form"
   },
   "source": [
    "#@title Category names for COCO.\n",
    "categories_str = '{\"categories\": [{\"supercategory\": \"person\",\"id\": 1,\"name\": \"person\"},{\"supercategory\": \"vehicle\",\"id\": 2,\"name\": \"bicycle\"},{\"supercategory\": \"vehicle\",\"id\": 3,\"name\": \"car\"},{\"supercategory\": \"vehicle\",\"id\": 4,\"name\": \"motorcycle\"},{\"supercategory\": \"vehicle\",\"id\": 5,\"name\": \"airplane\"},{\"supercategory\": \"vehicle\",\"id\": 6,\"name\": \"bus\"},{\"supercategory\": \"vehicle\",\"id\": 7,\"name\": \"train\"},{\"supercategory\": \"vehicle\",\"id\": 8,\"name\": \"truck\"},{\"supercategory\": \"vehicle\",\"id\": 9,\"name\": \"boat\"},{\"supercategory\": \"outdoor\",\"id\": 10,\"name\": \"traffic light\"},{\"supercategory\": \"outdoor\",\"id\": 11,\"name\": \"fire hydrant\"},{\"supercategory\": \"outdoor\",\"id\": 13,\"name\": \"stop sign\"},{\"supercategory\": \"outdoor\",\"id\": 14,\"name\": \"parking meter\"},{\"supercategory\": \"outdoor\",\"id\": 15,\"name\": \"bench\"},{\"supercategory\": \"animal\",\"id\": 16,\"name\": \"bird\"},{\"supercategory\": \"animal\",\"id\": 17,\"name\": \"cat\"},{\"supercategory\": \"animal\",\"id\": 18,\"name\": \"dog\"},{\"supercategory\": \"animal\",\"id\": 19,\"name\": \"horse\"},{\"supercategory\": \"animal\",\"id\": 20,\"name\": \"sheep\"},{\"supercategory\": \"animal\",\"id\": 21,\"name\": \"cow\"},{\"supercategory\": \"animal\",\"id\": 22,\"name\": \"elephant\"},{\"supercategory\": \"animal\",\"id\": 23,\"name\": \"bear\"},{\"supercategory\": \"animal\",\"id\": 24,\"name\": \"zebra\"},{\"supercategory\": \"animal\",\"id\": 25,\"name\": \"giraffe\"},{\"supercategory\": \"accessory\",\"id\": 27,\"name\": \"backpack\"},{\"supercategory\": \"accessory\",\"id\": 28,\"name\": \"umbrella\"},{\"supercategory\": \"accessory\",\"id\": 31,\"name\": \"handbag\"},{\"supercategory\": \"accessory\",\"id\": 32,\"name\": \"tie\"},{\"supercategory\": \"accessory\",\"id\": 33,\"name\": \"suitcase\"},{\"supercategory\": \"sports\",\"id\": 34,\"name\": \"frisbee\"},{\"supercategory\": \"sports\",\"id\": 35,\"name\": \"skis\"},{\"supercategory\": \"sports\",\"id\": 36,\"name\": \"snowboard\"},{\"supercategory\": \"sports\",\"id\": 37,\"name\": \"sports ball\"},{\"supercategory\": \"sports\",\"id\": 38,\"name\": \"kite\"},{\"supercategory\": \"sports\",\"id\": 39,\"name\": \"baseball bat\"},{\"supercategory\": \"sports\",\"id\": 40,\"name\": \"baseball glove\"},{\"supercategory\": \"sports\",\"id\": 41,\"name\": \"skateboard\"},{\"supercategory\": \"sports\",\"id\": 42,\"name\": \"surfboard\"},{\"supercategory\": \"sports\",\"id\": 43,\"name\": \"tennis racket\"},{\"supercategory\": \"kitchen\",\"id\": 44,\"name\": \"bottle\"},{\"supercategory\": \"kitchen\",\"id\": 46,\"name\": \"wine glass\"},{\"supercategory\": \"kitchen\",\"id\": 47,\"name\": \"cup\"},{\"supercategory\": \"kitchen\",\"id\": 48,\"name\": \"fork\"},{\"supercategory\": \"kitchen\",\"id\": 49,\"name\": \"knife\"},{\"supercategory\": \"kitchen\",\"id\": 50,\"name\": \"spoon\"},{\"supercategory\": \"kitchen\",\"id\": 51,\"name\": \"bowl\"},{\"supercategory\": \"food\",\"id\": 52,\"name\": \"banana\"},{\"supercategory\": \"food\",\"id\": 53,\"name\": \"apple\"},{\"supercategory\": \"food\",\"id\": 54,\"name\": \"sandwich\"},{\"supercategory\": \"food\",\"id\": 55,\"name\": \"orange\"},{\"supercategory\": \"food\",\"id\": 56,\"name\": \"broccoli\"},{\"supercategory\": \"food\",\"id\": 57,\"name\": \"carrot\"},{\"supercategory\": \"food\",\"id\": 58,\"name\": \"hot dog\"},{\"supercategory\": \"food\",\"id\": 59,\"name\": \"pizza\"},{\"supercategory\": \"food\",\"id\": 60,\"name\": \"donut\"},{\"supercategory\": \"food\",\"id\": 61,\"name\": \"cake\"},{\"supercategory\": \"furniture\",\"id\": 62,\"name\": \"chair\"},{\"supercategory\": \"furniture\",\"id\": 63,\"name\": \"couch\"},{\"supercategory\": \"furniture\",\"id\": 64,\"name\": \"potted plant\"},{\"supercategory\": \"furniture\",\"id\": 65,\"name\": \"bed\"},{\"supercategory\": \"furniture\",\"id\": 67,\"name\": \"dining table\"},{\"supercategory\": \"furniture\",\"id\": 70,\"name\": \"toilet\"},{\"supercategory\": \"electronic\",\"id\": 72,\"name\": \"tv\"},{\"supercategory\": \"electronic\",\"id\": 73,\"name\": \"laptop\"},{\"supercategory\": \"electronic\",\"id\": 74,\"name\": \"mouse\"},{\"supercategory\": \"electronic\",\"id\": 75,\"name\": \"remote\"},{\"supercategory\": \"electronic\",\"id\": 76,\"name\": \"keyboard\"},{\"supercategory\": \"electronic\",\"id\": 77,\"name\": \"cell phone\"},{\"supercategory\": \"appliance\",\"id\": 78,\"name\": \"microwave\"},{\"supercategory\": \"appliance\",\"id\": 79,\"name\": \"oven\"},{\"supercategory\": \"appliance\",\"id\": 80,\"name\": \"toaster\"},{\"supercategory\": \"appliance\",\"id\": 81,\"name\": \"sink\"},{\"supercategory\": \"appliance\",\"id\": 82,\"name\": \"refrigerator\"},{\"supercategory\": \"indoor\",\"id\": 84,\"name\": \"book\"},{\"supercategory\": \"indoor\",\"id\": 85,\"name\": \"clock\"},{\"supercategory\": \"indoor\",\"id\": 86,\"name\": \"vase\"},{\"supercategory\": \"indoor\",\"id\": 87,\"name\": \"scissors\"},{\"supercategory\": \"indoor\",\"id\": 88,\"name\": \"teddy bear\"},{\"supercategory\": \"indoor\",\"id\": 89,\"name\": \"hair drier\"},{\"supercategory\": \"indoor\",\"id\": 90,\"name\": \"toothbrush\"}]}'\n",
    "categories_dict = json.loads(categories_str)\n",
    "categories_dict = {c['id']: c for c in categories_dict['categories']}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "BYgT9kpDUGwi",
    "outputId": "3c5086bd-ac81-4ffd-a9b3-6d0bfa084655",
    "cellView": "form"
   },
   "source": [
    "#@title Load an image.\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg' #@param [\"http://images.cocodataset.org/val2017/000000039769.jpg\", \"http://images.cocodataset.org/val2017/000000210273.jpg\", \"http://images.cocodataset.org/val2017/000000224664.jpg\", \"http://images.cocodataset.org/val2017/000000018380.jpg\", \"http://images.cocodataset.org/val2017/000000470924.jpg\", \"http://images.cocodataset.org/val2017/000000309391.jpg\", \"http://images.cocodataset.org/val2017/000000191845.jpg\"] {allow-input: true}\n",
    "im = Image.open(requests.get(url, stream=True).raw)\n",
    "im"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 657
    },
    "id": "TxRTwqKEXfNf",
    "outputId": "2f1093b0-0e2b-41e5-887f-d436f4f68239",
    "cellView": "form"
   },
   "source": [
    "#@title Inference. (This can be slow the first time as it builds the inference graph.)\n",
    "\n",
    "num_instances_to_generate = 10 #@param\n",
    "min_score_thresh = 0.7 #@param\n",
    "\n",
    "# Build inference graph.\n",
    "task.config.task.max_instances_per_image_test = num_instances_to_generate\n",
    "@tf.function\n",
    "def infer(model, preprocessed_outputs):\n",
    "  return task.infer(model, preprocessed_outputs)\n",
    "\n",
    "# Construct features and dummy labels.\n",
    "im = np.array(im)\n",
    "features = {\n",
    "    'image': tf.image.convert_image_dtype(im, tf.float32),\n",
    "    'image/id': 0, # dummy image id.\n",
    "    'orig_image_size': tf.shape(im)[0:2],\n",
    "}\n",
    "labels = {\n",
    "    'label': tf.zeros([1], tf.int32),\n",
    "    'bbox': tf.zeros([1, 4]),\n",
    "    'area': tf.zeros([1]),\n",
    "    'is_crowd': tf.zeros([1]),\n",
    "}\n",
    "\n",
    "features, labels = data_utils.preprocess_eval(\n",
    "    features,\n",
    "    labels,\n",
    "    max_image_size=config.model.image_size,\n",
    "    max_instances_per_image=1)\n",
    "\n",
    "# Batch features and labels.\n",
    "features = {\n",
    "    k: tf.expand_dims(v, 0) for k, v in features.items()\n",
    "}\n",
    "labels = {\n",
    "    k: tf.expand_dims(v, 0) for k, v in labels.items()\n",
    "}\n",
    "\n",
    "# Inference.\n",
    "preprocessed_outputs = (features['image'], None, (features, labels))\n",
    "infer_outputs = infer(model, preprocessed_outputs)\n",
    "_, pred_seq, _ = infer_outputs\n",
    "results = task.postprocess_tpu(*infer_outputs)\n",
    "\n",
    "# Visualization.\n",
    "(images, _, pred_bboxes, _, pred_classes, scores, _, _, _, _, _) = results\n",
    "vis = vis_utils.visualize_boxes_and_labels_on_image_array(\n",
    "    image=tf.image.convert_image_dtype(images[0], tf.uint8).numpy(),\n",
    "    boxes=pred_bboxes[0].numpy(),\n",
    "    classes=pred_classes[0].numpy(),\n",
    "    scores=scores[0].numpy(),\n",
    "    category_index=categories_dict,\n",
    "    use_normalized_coordinates=True,\n",
    "    min_score_thresh=min_score_thresh,\n",
    "    max_boxes_to_draw=100)\n",
    "\n",
    "Image.fromarray(vis)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EECOIbXSX0rX"
   },
   "source": [
    ""
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Pix2Seq Inference (Object Detection).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
